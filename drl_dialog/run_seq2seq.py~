"""
Author: Heejin Chloe Jeong
Affiliation: University of Pennsylvania

DRL - Seq2Seq model for version 1.0 
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
import os
import random
import sys
import time
import pickle
import pdb

import numpy as np
from six.moves import xrange  # pylint: disable=redefined-builtin
import tensorflow as tf
import seq2seq_model_rl as s2s_mdl

tf.app.flags.DEFINE_float("learning_rate", 0.5, "Learning rate.")
tf.app.flags.DEFINE_float("learning_rate_decay_factor", 0.99,
                          "Learning rate decays by this much.")
tf.app.flags.DEFINE_float("max_gradient_norm", 5.0,
                          "Clip gradients to this norm.")
tf.app.flags.DEFINE_integer("batch_size", 64,
                            "Batch size to use during training.")
tf.app.flags.DEFINE_integer("layer_size", 1024, "Size of each model layer.")
tf.app.flags.DEFINE_integer("num_layers", 3, "Number of layers in the model.")
tf.app.flags.DEFINE_integer("vocab_size", 8038, "vocabulary size.")
tf.app.flags.DEFINE_string("data_dir", "/tmp", "Data directory")
tf.app.flags.DEFINE_string("train_dir", "/tmp", "Training directory.")
tf.app.flags.DEFINE_integer("max_train_data_size", 0,
                            "Limit on the size of training data (0: no limit).")
tf.app.flags.DEFINE_integer("steps_per_checkpoint", 10,
                            "How many training steps to do per checkpoint.")
tf.app.flags.DEFINE_boolean("decode", False,
                            "Set to True for interactive decoding.")
tf.app.flags.DEFINE_boolean("self_test", False,
                            "Run a self-test if this is set to True.")
tf.app.flags.DEFINE_integer("max_utterance_len",80,"the length of the maximum utterance length")

FLAGS = tf.app.flags.FLAGS

PAD_ID = 0
GO_ID = 2
EOS_ID = 8037
UNK_ID = 1

"""
Temporarily, index changing is necessary:
Currently, 
idx2w[0] = '_'
idx2w[1] = '<unk>'
idx2w[2] = '<go>'
idx2w[3] = 'i'

"""

BOC = [[GO_ID]] # Beginning of Conversation Vector!
_buckets = [(2*FLAGS.max_utterance_len, FLAGS.max_utterance_len)]

def read_data(data_path, max_size = None):
	removing_ID = [28] # 28: um 
	data_set = [[] for _ in _buckets]

	for f_name in os.listdir(data_path):
		dialogue = np.load(os.path.join(data_path, f_name))
		dialogue = BOC + np.ndarray.tolist(dialogue)
		print("======== reading data ======== file: "+f_name)
		for k in xrange(len(dialogue)-2):
			source_ids = [x for x in dialogue[k] if not(x in removing_ID)] + [x for x in dialogue[k+1] if not(x in removing_ID)]
			target_ids = [x for x in dialogue[k+2] if not(x in removing_ID)]
			# If source_ids or target_ids is empty, add "um"
			if not(source_ids):
				source_ids = [28]
			if not(target_ids):
				target_ids = [28]
			target_ids.append(EOS_ID)

			for bucket_id, (source_size, target_size) in enumerate(_buckets):
				if len(source_ids) < source_size and len(target_ids) < target_size:
					data_set[bucket_id].append([source_ids,target_ids])
					break

	return data_set

def create_model(session, forward_only):
  """Create translation model and initialize or load parameters in session.
  If there is any problem on checkpoint, see "tensorflow/contrib/framework/python/framework/checkpoint_util.py"

	"""
  model = s2s_mdl.Seq2SeqModel(
      FLAGS.vocab_size, _buckets,
      FLAGS.layer_size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,
      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor, forward_only=forward_only)

  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)
  if FLAGS.decode:
    print("Reading model parameters from %s" % ckpt.model_checkpoint_path)
    model.saver.restore(session, ckpt.model_checkpoint_path)
  else:
    print("Created model with fresh parameters.")
    session.run(tf.global_variables_initializer())
  return model


def train():
	with tf.Session() as sess:
		print("Creating %d layers of %d units." % (FLAGS.num_layers,FLAGS.layer_size))
		model = create_model(sess,False)

		print ("Reading development and training data (limit: %d)."
           % FLAGS.max_train_data_size)

		train_set = read_data(os.path.join(FLAGS.data_dir,"train"),FLAGS.max_train_data_size)
		dev_set = read_data(os.path.join(FLAGS.data_dir,"dev"))
		train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]
		train_total_size = float(sum(train_bucket_sizes))

		# A bucket scale is a list of increasing numbers from 0 to 1 that we'll use
		# to select a bucket. Length of [scale[i], scale[i+1]] is proportional to
		# the size if i-th training bucket, as used later.
		train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]

		# This is the training loop.
		step_time, loss = 0.0, 0.0
		current_step = 0
		previous_losses = []

		while True:
			# There is only one bucket...
			bucket_id = 0
			start_time = time.time()
			encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)
			_, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)
			step_time += (time.time()-start_time)/FLAGS.steps_per_checkpoint
			loss += step_loss / FLAGS.steps_per_checkpoint
			current_step += 1

			# For saving checkpoint, print statistics and run evals.
			if current_step % FLAGS.steps_per_checkpoint == 0:
				# Print statistics for the previous epoch.
				perplexity = math.exp(loss) if loss < 300 else float('inf')
				print("global step %d learning rate %.4f step-time %.2f perplexity" "%.2f" % 
    			(model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))
    		
				# Decrease the learning rate if no improvement was seen over last 3 times.
				if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):
					sess.run(model.learning_rate_decay_op)
				previous_losses.append(loss)

				# Save checkpoint and zero timer and loss.
				checkpoint_path = os.path.join(FLAGS.train_dir, "seq2seq.ckpt")
				model.saver.save(sess, checkpoint_path, global_step = model.global_step)
				step_time, loss = 0.0, 0.0

				# Run evals on development set and print their perplexity.

				encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id )
				_, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)
				eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')
				print(" eval: bucket %d perplexity %.2f" % (bucket_id, eval_ppx))
				sys.stdout.flush()



def main(_):
  if FLAGS.self_test:
    self_test()
  elif FLAGS.decode:
    decode()
  else:
    train()

if __name__ == "__main__":
  tf.app.run()
