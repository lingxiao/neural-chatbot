In this section explain the hiearchical recurrent neural net (HRED) model proposed by \cite{Serban} and \cite{Sordoni}, the problem they posed and how HRED solves these problems. In the original paper by \cite{Sordoni}, HRED was proposed as a model that could generate context-senstitive queries, then \cite{Serban} adapted it to train an end-to-end history-aware conversation model.

\subsection{Recurrent Neural Net}

Now we wish to review recurrent neural net (RNN) in some detail. We denote a dialogue as a sequence of $M$ utterances $U_m$: $D = \{U_1, \ldots, U_M\}$ between two speakers, each utterance contains $N_m$ tokens: $U_m = \{w_{m,1},\ldots,w_{m,N_m}\}$. $\cite{Serban}$ allowed the random variable $w_{m,n}$ to range over the vocabular and ``speech acts", although in our case we only considered words. Next they defined a distribution $P$ with parameter $\theta$ over the set of all possible dialogues of any length, and factorized $P$ by:
    \begin{align*}
        P_{\theta}(U_1, \ldots, U_M) &= \prod_{m=1}^M P_{\theta}(U_m | U_{<m}) \\
                                     &= \prod_{m=1}^M \prod_{n=1}^{N_m} P_{\theta} (w_{m,n}|w_{m,<n}, U_{<m}),
    \end{align*}

where $U_{<m} = \{U_1, \ldots, U_{m-1}\}$. In other words, the conditional probability of the current word is only a function of previous words in the utterance and previous utterances. Next, \cite{Serban} represents $P_{\theta}$ with:
    \begin{align*}
        P_{\theta} (w_{n+1} = v | w_{\leq n}) = \frac{\exp(g(h_n,v))}{\sum_{v'} \exp(g(h_n, v'))},
    \end{align*}

where $h_n \in \mathbbm{R}^{d_h}$ is a hidden state computed by a recurrent neural net:

    \begin{align*}
        &h_n = tanh (H h_{n-1} + I_{w_n}) \\
        &g(h_n, v) = O^{T}_{w_n} h_n,
    \end{align*}

where $I \in \mathbbm{R}^{d_h \times |V|}$ is the input word embeding. Again note that the hidden state is only a function of the past. 

\subsection{Hierarchical Recurrent Neural Net}

HRED augments the basic RNN model by explicitly learning a transition function over the hidden dynamic of the conversation or  ``session". Serban and Sordoni learned this transition function using a session level RNN. Specifcially, this RNN predicts the next utterance by:
    \begin{align*}
        P(U_m|U_{<m}) &= \prod_{n=1}^{N_m} P(w_n | w_{<n-1}, U_{m-1}) \\
                      &= \frac{\exp(o_{v}^T w(d_{m,n-1}, w_{{m,n-1}}))}{\sum_{k} \exp(o_{k}^Tw(d_{m,n-1}, w_{m,n-1}))},
    \end{align*}

where $w(d_{m,n-1}, w_{m,n-1}) = H_o d_{m,n-1} + E_o w_{m,n-1} + b_o,$ so that the next utterance is a function of all pervious utterances. In summary, in HRED each utterance is a function of the previous utterance, and each word in the utterance is a function of all previous words in the utterance. A session level RNN learns the transition between utterances vector, and sentence level RNN learns the transition between words. 

\subsection{Learning}

HRED is trained end-to-end by maximizing the log-likelihood of the whole session $S$:

    \begin{align*}
        Loss(S) &= \sum_{m=1}^M log P(U_m|U_{<m}) \\
                &= \sum_{m=1}^M \sum_{n=1}^{N_m} log P(w_{m,n} | w_{m,<n}, U_{<m}).
    \end{align*}

We train the model by dividing the data set into sessions with four utterances each, the vocabulary size was set to be 50,005, and the maximum sentence length was 50 tokens.



































