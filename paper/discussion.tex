In this project, we implemented HRED and DRL-SEQ2SEQ model with the OpenSubtitle Dataset. We trained HRED for five days but saw no dramatic qualitative improvements after two day. Futhermore, there is no qualitative evidence that HRED is keeping track of conversational state. There are several possible reasons for this. First the data set may not have been appropriately preprocessed for our task, looking at the last three sentences of table 2, it is clear that the second to last two sentences are spoken by the same person. Future work would certainly need to be much more careful in their annotation of speakers if they choose to use this data set. Furthermore, since HRED is hypothesized to improve chatbot performance by keeping track of a session state, future data sets should ensure some session state exists by inspection. For example looking through the movie corpus, often times it is not clear there is such a state, especially if there are narrator interjections in the script, or scene cuts. Finally, the implementation of HRED may also limit the model's ability to learn full conversations. Since the current implementation of HRED assumes the entire session must be passed in as one vector, in practice we had to limit our sessions to four turns each, and each utterance could be at most 50 tokens long. This is a very stringent criteria in practice, since a conversation with explicit state, ie: greeting, middle and finally the good-bye is certainly longer than four turns. On a more general note, the notion that there could be enough data to place a distribution over the set of all conversations of arbitrary length may be unrealistic unless we are considering the most stringent definition of ``conversation" possible. Restricting HRED to a limited domain (ie by topic) may alleviate such issues. 

For the implementation of DRL-SEQ2SEQ, our mutual information model generated poor responses and we could see that the model quickly diverges to wrong outcomes during training. There are several possible reasons for this poor performance. First, we did not use the curriculum strategy, but the strategy may play a critical role in RL learning since the reward functions do not perfectly describe quality of responses. Second, we used 0.5 for the learning rate of SGD (the default value in the tensorflow seq2seq model), but it may have been too large because the performance changed very quickly. Lastly, not using the baseline strategy may also have affected the performance since it is known that the strategy reduces the variance of PGO. 