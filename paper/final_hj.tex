\def\year{2016}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai16}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{bbm}
%\usepackage{graphics}
\usepackage{subcaption}
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{tabularx, adjustbox}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\real}{\mathbb{R}}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation)
/Author (Heejin Jeong, Xiao Ling)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Deep Reinforcement Learning with Hierarchical Recurrent Encoder-Decoder for Conversation }
\author{Heejing Jeong \and Xiao Ling\\
  {\tt [heejinj, lingxiao]@seas.upenn.edu}}
\maketitle

\section{Introduction}
\input{intro.tex}

\section{Hierarchical Recurrent Encoder Decoder}
\input{hred.tex}

\section{Deep Reinforcement Learning for Dialog Generation}
\input{drl.tex}

\section{Experiment}

\begin{figure}[bt!]
    \centering
    \includegraphics[width=0.48\textwidth]{mi_architecture} 
    \caption{\small SEQ2SEQ and MI model Code Architecture}
    \label{fig:mi_architecture}
 \end{figure}

\subsection{Dataset}
\input{dataset.tex}

\subsection{Implementation}
Although it is not common to explain code structures in an academic paper, we explicitly mentioned functions and files in tensorflow we used as well as those written by us in this section.  
For the SEQ2SEQ model, we applied Sequence to sequence with attention mechanism and used GRU cells instead of LSTM cells. We used 3 layers and 512 units for all models. We implemented models in tensorflow version 1.0.1. We used \texttt{embedding\_attention\_seq2seq()} function in \texttt{seq2seq.py} and related functions/codes written in tensorflow. We also utilized \texttt{seq2seq\_model.py} in tensorflow version 0.12.x as a bottom line.\\
The \texttt{seq2seq\_model.py} is structured as follow (see Fig.\ref{fig:mi_architecture}): when the model is initialized, three main lists of placeholders are constructed - encoder, decoder, and weight. The length of the lists are encoder size, decoder size, and decoder size, respectively. Each element of a list is a tensor with a shape [batch size, None]. Variables are also built and initialized. In each step of training, batch size data are sampled in a given training dataset and converted into a proper format in the function \texttt{get\_batch()} by operations such as padding, adding GO id, sizing, etc. Then, these are fed to the placeholders. Output logits are computed through \texttt{embedding\_attention\_seq2seq()} function and losses are computed using a softmax loss function. Then, variables in the encoder-decoder recurrent neural network (RNN) are updated. \\
The mutual information model requires a pre-trained SEQ2SEQ model and a pre-trained backward SEQ2SEQ model for its initialization and for computing mutual information score. The model first builds three placeholder lists - concatenated states (e.g. $u_{t-1},u_t$), the most recent state (e.g. $u_t$) and weights for the backward model. Instead of sampling several action candidates given an input from the policy probability distribution $p_{RL}$ as presented in the paper, we sampled a batch of inputs (size of 64) and selected the best action of each sampled state based on their probability outcomes. These states (or inputs), actions, and weights become inputs to the pre-trained SEQ2SEQ and the pre-trained backward SEQ2SEQ model, and we obtain log probabilities from each model. Then, the mutual information score and the gradient in Eq.\ref{eq:gradient} are computed. According to the paper \cite{Li}, stochastic gradient descent (SGD) was applied to the objective function \ref{eq:obj_func}. However, since SGD is for minimization and the objective function has to be maximized, we used SGA instead to update variables of the encoder-decoder RNN. In this project, we implemented mutual information model without the curriculum and the baseline strategies.

\section{Results}
\subsection{DRL-SEQ2SEQ}
The training results of the backward SEQ2SEQ model after 109000 steps are presented in Table.\ref{table:backward_seq2seq}. Since it is a backward model, the bot generates the most likely utterance that will have the input utterance as its response. The examples in the Good section show good performance of the model. The generated responses in the Bad section do not make sense. The model tends to generate certain responses a lot such as "i am going to go to the bathroom", or to repeat all or some words in an input utterance. The training results of the SEQ2SEQ model with concatenated utterances after 107000 steps are presented in Table.\ref{table:seq2seq}. Each row of the table is one conversation, since the model consider two previous sentences. However, from the results, it was not hard to see that the model responses considering both two previous utterances. Also, it generates "i am sorry" or "no" many times. For example, for almost any human input utterance starting with "can you", it generates "no". \\
The results of the mutual information model were not good at all. First of all, since the mutual information score is averaged by the number of words in a sentence and each term is negative (log probability), we could see that the shorter a response is, the higher its score is. Also, it tends to generates lots of repeated words in a sentence. The example is shown in \ref{table:mi_01}. Therefore, we added an additional reward for the number of words, $N_a$ of an output sentence $a$ and penalties for the repeated words and for "i am sorry" or "no" response:
\begin{align}
    J(\theta) =&\; \mathbf{E} [-m(\hat{a},[u_{t-1},u_t])-c_l N_a \nonumber \\
     &\; + c_r \sum_{1\leq i < N_a} \mathbbm{1}(w_i^{(a)} == w_{i+1}^{(a)}) \nonumber \\ 
     &\; +c_d \mathbbm{1}(a == \text{"no" or "i am sorry"})]
\end{align}
where $c_l$, $c_r$, and $c_d$ are constant weights and $w_i^{(a)}$ is the $i$th word in a sentence $a$. However, those additional reward and penalties didn't improve the overall quality of responses. The Table.\ref{table:mi_02} shows an example of after 20 steps and the Table.\ref{table:mi_03} shows an example after training for 540 steps. Since we considered only the next word in the repeated-words penalty, the generated response repeat a set of words. Also, the outcomes tend to be unnecessarily long.

\begin{table}[bt!]
    \centering
    \small
    \caption{\small Backward Seq2Seq Model Examples}
    \begin{tabularx}{0.48\textwidth}{c|rl}
    	\hline
      \hline
        & \textbf{human:} & i do not know man\\
		& \textbf{bot:} & what do you think\\		
& \textbf{human:} & that s amazing \\
& \textbf{bot:} & wow \\
& \textbf{human:} & that works best for me \\
& \textbf{bot:} & i will be right back \\
Good & \textbf{human:} & I love you too \\
& \textbf{bot:} & i love you \\
& \textbf{human:} & i know i am hot \\
& \textbf{bot:} & i am a little nervous \\
& \textbf{human:} & i am done with you \\
& \textbf{bot:} & i am a fucking bitch \\
\hline
& \textbf{human:} & i am going to go to the bathroom\\
& \textbf{bot:} & i am going to go to the bathroom \\
& \textbf{human:} & this is weird \\
& \textbf{bot:} & i am going to be a great man \\
Bad & \textbf{human:} & okay\\
& \textbf{bot:} & i am going to go to the bathroom\\
& \textbf{human:} & you look gorgeous \\
& \textbf{bot:} & i am going to go see the kitchen \\
& \textbf{human:} & fine\\
& \textbf{bot:} & i am fine\\
      \hline
    \end{tabularx}
    \label{table:backward_seq2seq}
\end{table}

\begin{table}[bt!]
    \centering
    \small
    \caption{\small Seq2Seq Model Examples}
    \begin{tabularx}{0.48\textwidth}{rl}
    	\hline
      \hline
        \textbf{human:} & this is bad \\
	\textbf{bot:} & this is a joke \\
	\textbf{human:} & what joke \\
	\textbf{bot:} & i do not know \\
	\textbf{human:} & what are you doing \\
	\textbf{bot:} & nothing \\
	\hline
	\textbf{human:} & you are a loser\\
	\textbf{bot:} & you are a real joke you know \\
	\textbf{human:} & what is a real joke \\
	\textbf{bot:} & i do not know \\
	\hline
	\textbf{human:} & how old are you \\
	\textbf{bot:} & eleven \\
	\textbf{human:} & you are so young boy \\
	\textbf{bot:} & i am sorry \\
	\textbf{human:} & it's totally okay \\
	\textbf{bot:} & i am sorry \\
	\textbf{human:} & what are you up to this weekend \\
	\textbf{bot:} & i am sorry \\
	\hline
	\textbf{human:} & where are you from \\
	\textbf{bot:} & where are you going \\
	\textbf{human:} & i am going home \\ 
	\textbf{bot:} & i am going to get you out of here \\
	\textbf{human:} & thank you but where \\
	\textbf{bot:} &  what is this \\
	\textbf{human:} & nothing \\
	\textbf{bot:} & i am sorry \\
      \hline
    \end{tabularx}
    \label{table:seq2seq}
\end{table}

\begin{table}[bt!]
    \centering
    \small
    \caption{\small Mutual Information Model Results}
    \begin{tabularx}{0.48\textwidth}{X}
        \hline \hline
        The highest probability response among actions in one batch after 10 steps \\
        \hline    
        \textbf{bot:} i am flattered flattered truancy truancy truancy truancys truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy truancy  truancy soviets soviets soviets soviets soviets soviets soviets soviets soviets soviets \\
        \hline \hline
        The lowest probability response in one batch after 10 steps \\
        \hline
        \textbf{bot:} i am \\
        \hline \hline  
    \end{tabularx}
    \label{table:mi_01}
\end{table}
\begin{table}[bt!]
    \centering
    \small
    \caption{\small Mutual Information Model with additional reward/penalty}
    \begin{tabularx}{0.48\textwidth}{X}
        \hline \hline
        The highest probability response among actions in one batch after 20 steps \\
        \hline    
        \textbf{bot:} i have loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou loveyou \\
        \hline \hline
        The lowest probability response in one batch after 10 steps \\
        \hline
        \textbf{bot:} yeah \\
        \hline \hline    
    \end{tabularx}
    \label{table:mi_02}
\end{table}

\begin{table}[bt!]
    \centering
    \small
    \caption{\small Mutual Information Model with additional reward/penalty after 540 steps}
    \begin{tabularx}{0.48\textwidth}{rX}
        \hline \hline
        \textbf{human:} & how old are you?    \\
        \textbf{bot:} & 000 toppy * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid *\\
        \textbf{human:} & how are you doing? \\
        \textbf{bot:} & fiancee pyramid * didit * didit * didit * didit * didit * didit * didit * didit * didit * didit * didit * didit *\\
        \textbf{human:} & okay? \\
        \textbf{bot:} & fiancee pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * didit pyramid * \\
        \hline \hline    
    \end{tabularx}
    \label{table:mi_03}
\end{table}


\subsection{HRED}

In this section we discuss the results for HRED. It was trained for 5 days on a GeForce GTX TITAN X GPU. The results after five hours are displayed in table 8. We keep the example brief and show an elucidating exmaple: after five hours the model appears to have acquired a basic grammar, but have a tendancy to output repeated words. Next we direct our attention to table 8, where we show the results after training for 72 hours. We observed a marked decrease in sentences with repeated words, and the table shows one example of a response that appears to have some ``semantic" relationship with the question asked. We ultimately trained the models for 5 days, and the results are displayed in table 10. In general, the results after 5 days do not appear qualitatively better than results after just 72 hours. In particular, we see no evidence that HRED is tracking ``some" conversation state. 

\begin{table}[bt!]
    \centering
    \small
    \caption{\small HRED after 5 hours}
    \begin{tabularx}{0.48\textwidth}{rX}
        \hline \hline
        \textbf{human:} & to be quite honest it is not very clear for me either?    \\
        \textbf{bot:} & what let s a  bitch and i m know you s gone black to you in you in you to you to you out of it to black to move to me *\\
        \hline \hline    
    \end{tabularx}
    \label{table:mi_03}
\end{table}

\begin{table}[bt!]
    \centering
    \small
    \caption{\small HRED after 5 hours}
    \begin{tabularx}{0.48\textwidth}{rX}
        \hline \hline
        \textbf{human:} & do you have any weapons explosives. any history of violent conduct \\
        \textbf{bot:} & a very interracial couples you re not gonna die you that s complicated \\
        \hline \hline    
    \end{tabularx}
    \label{table:mi_03}
\end{table}

\begin{table}[bt!]
    \centering
    \small
    \caption{\small HRED after 5 hours}
    \begin{tabularx}{0.48\textwidth}{rX}
        \hline \hline
        \textbf{human:} & i can t remember anything \\
        \textbf{bot:} & i got where the target and make preparation on froebe in jill \\
        \textbf{human:} & i m a guinea pig \\
        \textbf{bot: } & morning the problem your brutality \\
        \hline \hline    
    \end{tabularx}
    \label{table:mi_03}
\end{table}


\section{Discussion}
\input{discussion.tex}
% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{cis700}
\bibliographystyle{aaai}

\end{document}
