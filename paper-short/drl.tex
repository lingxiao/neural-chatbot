\section{DRL with HRED}
Our model differs from DRL-SEQ2SEQ model in three aspects. First, instead of using two previous dialogue utterances to define a state, we define a state $s_t$ at time $t$ as a hidden state $c_t$ of the dialogue level RNN at the time, $s_t=c_t = f(c_{t-1},h_T^{(t)})$, where $h_T^{(t)}$ is the last hidden state of the $t$th utterance and $f$ is a parametrized non-linear function. We use the same definition for actions - generating a dialogue utterance. Thus, a RL policy is defined as $\pi(a_t|s_t) =p_{RL}(u_{t+1}|c_t)$. Second, we pre-train a model using HRED to initialize the RL policy $\pi$. Finally, we replace the pre-trained SEQ2SEQ model with the pre-trained HRED model in reward functions of RL. 